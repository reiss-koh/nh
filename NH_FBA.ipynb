{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7770390f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 주제명: Convoluted Autoencoder-based Dimension Reduced Sociodemographic Clustering of Digital Finance Customers\n",
    "# ^ 더 마땅한걸로 인수님이 바꿔주실 거라 생각합니다 ㅎㅎ\n",
    "\n",
    "### 팀명: Fine Bigdata Analysts (FBA)\n",
    "### 팀원1: 구본우 (KAIST 산업및시스템공학과)\n",
    "### 팀원2: 최인수 (KAIST 산업및시스템공학과)\n",
    "### 팀원3: 고우성 (Yonsei University 경제학과 컴퓨터과학과)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b55a08",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Import Packages \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0374a9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import nan\n",
    "from scipy import stats\n",
    "from datetime import date, datetime\n",
    "import investpy\n",
    "from pandas_datareader import data as pdr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a479c5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Global Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8552d2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296a389a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Global Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42952cbc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def isfloat(num):\n",
    "    try:\n",
    "        float(num)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def most_frequent(list):\n",
    "    return max(set(list), key = list.count)\n",
    "\n",
    "def can_convert_to_int(string):\n",
    "    try:\n",
    "        int(string)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d71783",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Global Tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78bd67d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "RAW_DATA = (\"cus_info\",\n",
    "            \"cus_account\")\n",
    "\n",
    "RAW_DATA1 = (\"kr_equity\",\n",
    "             \"os_equity\")\n",
    "\n",
    "DATA_PATH = (\"kr_equity\",\n",
    "             \"os_equity\",\n",
    "             \"cus_account\")\n",
    "\n",
    "DATA_PATH1 = (\"os_equity\",\n",
    "              \"cus_account\")\n",
    "\n",
    "DATA_PATH2 = (\"cus_account_R3\",\n",
    "              \"cus_assets_R1\",\n",
    "              \"kr_equity_R2\",\n",
    "              \"os_equity_R3\")\n",
    "\n",
    "DATA_PATH3 = (\"cus_account\",\n",
    "              \"cus_assets\",\n",
    "              \"kr_equity\",\n",
    "              \"os_equity\")\n",
    "\n",
    "DATA_PATH4 = (\"cus_account_R5\",\n",
    "              \"cus_assets_R5\",\n",
    "              \"kr_equity_R5\",\n",
    "              \"os_equity_R5\",\n",
    "              \"cus_info_R3\")\n",
    "\n",
    "ASSET_COLUMNS = (\"tot_aet_tld_rnd_202201\",\n",
    "                 \"tot_aet_tld_rnd_202202\",\n",
    "                 \"tot_aet_tld_rnd_202203\",\n",
    "                 \"tot_aet_tld_rnd_202204\",\n",
    "                 \"tot_aet_tld_rnd_202205\",\n",
    "                 \"tot_aet_tld_rnd_202206\")\n",
    "\n",
    "MODE_COLUMNS = (\"act_no\",\n",
    "                \"mts_mm_access_type\",\n",
    "                \"SEX_F\",\n",
    "                \"SEX_M\",\n",
    "                \"SEX_NA\",\n",
    "                \"cus_age_stn_cd\",\n",
    "                \"pft_amt_stn_cd\",\n",
    "                \"fst_act_opn_dt\",\n",
    "                \"LIFESTAGE_1\",\n",
    "                \"LIFESTAGE_2\",\n",
    "                \"LIFESTAGE_3\",\n",
    "                \"LIFESTAGE_4\",\n",
    "                \"LIFESTAGE_5\",\n",
    "                \"LIFESTAGE_6\",\n",
    "                \"LIFESTAGE_7\",\n",
    "                \"LIFESTAGE_8\",\n",
    "                \"LIFESTAGE_9\",\n",
    "                \"LIFESTAGE_10\",\n",
    "                \"LIFESTAGE_NA\",\n",
    "                \"tco_cus_grd_cd\",\n",
    "                \"tot_ivs_te_sgm_cd\",\n",
    "                \"HOLDINGS_TYPE_1.0\",\n",
    "                \"HOLDINGS_TYPE_2.0\",\n",
    "                \"HOLDINGS_TYPE_3.0\",\n",
    "                \"HOLDINGS_TYPE_4.0\",\n",
    "                \"HOLDINGS_TYPE_NA\",\n",
    "                \"loy_sgm_cd\",\n",
    "                \"SECTOR_1.0\",\n",
    "                \"SECTOR_2.0\",\n",
    "                \"SECTOR_3.0\",\n",
    "                \"SECTOR_4.0\",\n",
    "                \"SECTOR_5.0\",\n",
    "                \"SECTOR_6.0\",\n",
    "                \"SECTOR_7.0\",\n",
    "                \"SECTOR_8.0\",\n",
    "                \"SECTOR_9.0\",\n",
    "                \"SECTOR_10.0\",\n",
    "                \"SECTOR_11.0\",\n",
    "                \"SECTOR_12.0\",\n",
    "                \"SECTOR_13.0\",\n",
    "                \"SECTOR_14.0\",\n",
    "                \"SECTOR_15.0\",\n",
    "                \"SECTOR_16.0\")\n",
    "\n",
    "MAX_COLUMNS = (\"stk_pdt_hld_yn\",\n",
    "               \"ose_stk_pdt_hld_yn\",\n",
    "               \"mrz_pdt_tp_sgm_cd\",\n",
    "               \"mrz_mkt_dit_cd\",\n",
    "               \"aet_bse_stk_trd_tp_cd\",\n",
    "               \"bas_stk_trd_tp_cd\")\n",
    "\n",
    "DROP_COLUMNS = (\"stl_bse_itg_bnc_qty\",\n",
    "                \"stl_bse_fc_now_eal_amt\",\n",
    "                \"itg_byn_cns_qty\",\n",
    "                \"itg_sll_cns_qty\",\n",
    "                \"cns_bse_itg_bnc_qty\",\n",
    "                \"cns_bse_now_eal_amt\",\n",
    "                \"cns_bse_fc_now_eal_amt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910621ae",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data Process Parent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ada20b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DataProcess(object):\n",
    "    def __init__(self, data_path, excel_or_csv=\"\", data_path1=\"\", data_path2=\"\", data_path3=\"\", data_path4=\"\",\n",
    "                 data_path5=\"\", data_path6=\"\", data_path7=\"\", data_path8=\"\", data_path9=\"\"):\n",
    "\n",
    "        if excel_or_csv == \"csv\":\n",
    "            self.df = pd.read_csv(data_path, sep=',', encoding='unicode_escape')\n",
    "            self.df_list = [self.df]\n",
    "            if data_path1 != \"\":\n",
    "                self.df1 = pd.read_csv(data_path1, sep=',', encoding='unicode_escape')\n",
    "                self.df_list.append(self.df1)\n",
    "            if data_path2 != \"\":\n",
    "                self.df2 = pd.read_csv(data_path2, sep=',', encoding='unicode_escape')\n",
    "                self.df_list.append(self.df2)\n",
    "            if data_path3 != \"\":\n",
    "                self.df3 = pd.read_csv(data_path3, sep=',', encoding='unicode_escape')\n",
    "                self.df_list.append(self.df3)\n",
    "            if data_path4 != \"\":\n",
    "                self.df4 = pd.read_csv(data_path4, sep=',', encoding='unicode_escape')\n",
    "                self.df_list.append(self.df4)\n",
    "            if data_path5 != \"\":\n",
    "                self.df5 = pd.read_csv(data_path5, sep=',', encoding='unicode_escape')\n",
    "                self.df_list.append(self.df5)\n",
    "            if data_path6 != \"\":\n",
    "                self.df6 = pd.read_csv(data_path6, sep=',', encoding='unicode_escape')\n",
    "                self.df_list.append(self.df6)\n",
    "            if data_path7 != \"\":\n",
    "                self.df7 = pd.read_csv(data_path7, sep=',', encoding='unicode_escape')\n",
    "                self.df_list.append(self.df7)\n",
    "            if data_path8 != \"\":\n",
    "                self.df8 = pd.read_csv(data_path8, sep=',', encoding='unicode_escape')\n",
    "                self.df_list.append(self.df8)\n",
    "            if data_path9 != \"\":\n",
    "                self.df9 = pd.read_csv(data_path9, sep=',', encoding='unicode_escape')\n",
    "                self.df_list.append(self.df9)\n",
    "        else:\n",
    "            self.df = pd.read_excel(data_path)\n",
    "            self.df_list = [self.df]\n",
    "            if data_path1 != \"\":\n",
    "                self.df1 = pd.read_excel(data_path1)\n",
    "                self.df_list.append(self.df1)\n",
    "            if data_path2 != \"\":\n",
    "                self.df2 = pd.read_excel(data_path2)\n",
    "                self.df_list.append(self.df2)\n",
    "            if data_path3 != \"\":\n",
    "                self.df3 = pd.read_excel(data_path3)\n",
    "                self.df_list.append(self.df3)\n",
    "            if data_path4 != \"\":\n",
    "                self.df4 = pd.read_excel(data_path4)\n",
    "                self.df_list.append(self.df4)\n",
    "            if data_path5 != \"\":\n",
    "                self.df5 = pd.read_excel(data_path5)\n",
    "                self.df_list.append(self.df5)\n",
    "            if data_path6 != \"\":\n",
    "                self.df6 = pd.read_excel(data_path6)\n",
    "                self.df_list.append(self.df6)\n",
    "            if data_path7 != \"\":\n",
    "                self.df7 = pd.read_excel(data_path7)\n",
    "                self.df_list.append(self.df7)\n",
    "            if data_path8 != \"\":\n",
    "                self.df8 = pd.read_excel(data_path8)\n",
    "                self.df_list.append(self.df8)\n",
    "            if data_path9 != \"\":\n",
    "                self.df9 = pd.read_excel(data_path9)\n",
    "                self.df_list.append(self.df9)\n",
    "\n",
    "        self.df_len = len(self.df)\n",
    "\n",
    "    # dfs: data frames\n",
    "    # dps: data paths\n",
    "    def export(self, dps, excel_or_csv=\"\"):\n",
    "        if len(dps) == len(self.dfs):\n",
    "\n",
    "            if excel_or_csv == \"csv\":\n",
    "                for i in range(len(self.dfs)):\n",
    "                    self.dfs[i].to_csv(dps[i])\n",
    "                print(\"Exported To:\", dps)\n",
    "\n",
    "            else:\n",
    "                for i in range(len(self.dfs)):\n",
    "                    self.dfs[i].to_excel(dps[i])\n",
    "                print(\"Exported To:\", dps)\n",
    "\n",
    "        else:\n",
    "            print(\"Incorrect Number of Data Paths\")\n",
    "            print(\"Data Paths: \", len(dps))\n",
    "            print(\"Data Frames: \", len(self.dfs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5499d31e",
   "metadata": {},
   "source": [
    "## Data Processing Logic\n",
    "### After checking that each customer maps to a unique account, we map all other features based on customers' accounts. \n",
    "### We ensure that the machine learning algorithm does not learn spurious relationships due to the data. First, for all categorical data, we one hot encode them. Next, we appropriately handle non-available or \"99\" data points. Also, we handle symbolic numeric values such as \"mts_mm_access_type: 110000\" appropriately to proxy and represent the underlying feature. This is also the case for some dates.\n",
    "### We notice that there are some provided columns representing the same feature. We choose to omit the less accurate column for the column that best represents the customer's feature. We also omit features discretionarily through domain knowledge when we believe the signal(insight) to noise ratio is very low.\n",
    "### We discretionary group certain sets of values to proxy for a feature we believe is more meaningful than the non-processed column. This helps cluster the data into sociodemographic personas, which provides meaningful insight for customer targeting. \n",
    "### For intuitiveness, we reverse-order some numerically labeled features as we must look back at the features post-clustering.\n",
    "### We take a cross-sectional approach to our clustering instead of using a machine learning model suited for time-series. Therefore, we find an appropriate aggregation or representation method when transforming the time-series data into cross-sectional. We assume that six-month-month changes do not provide meaningful insight for a clustering algorithm. \n",
    "### We introduce new data in relation to existing observations and features, allowing us to create a new feature, \"Value Weighted Volatility\" that improves our understanding of the customers.\n",
    "### Lastly, we handle outliers and scale data through the MinMax approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4add5a4c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data Process Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497799b6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Check if Each Customer Maps to Unique Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccf81b2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class customerAccount(DataProcess):\n",
    "    def __init__(self, data_path, excel_or_csv=\"\"):\n",
    "        super().__init__(data_path=data_path, excel_or_csv=excel_or_csv)\n",
    "\n",
    "    def process(self):\n",
    "        self.df = self.df[[\"cus_no\", \"act_no\"]]\n",
    "\n",
    "        memory = {}\n",
    "\n",
    "        for i in range(len(self.df)):\n",
    "            if self.df.loc[i][\"cus_no\"] not in memory:\n",
    "                memory[self.df.loc[i][\"cus_no\"]] = [self.df.loc[i][\"act_no\"]]\n",
    "            elif self.df.loc[i][\"cus_no\"] in memory:\n",
    "                if self.df.loc[i][\"act_no\"] not in memory[self.df.loc[i][\"cus_no\"]]:\n",
    "                    memory[self.df.loc[i][\"cus_no\"]].append(self.df.loc[i])\n",
    "\n",
    "        self.df1 = pd.DataFrame()\n",
    "        self.df1[\"Customer\"] = [\"_\"] * len(memory)\n",
    "        self.df1[\"Account Number\"] = [\"_\"] * len(memory)\n",
    "\n",
    "        j = 0\n",
    "        for key, value in memory.items():\n",
    "            self.df1.at[j, \"Customer\"] = key\n",
    "            self.df1.at[j, \"Account Number\"] = len(value)\n",
    "            j += 1\n",
    "\n",
    "        for i in range(len(self.df1)):\n",
    "            if self.df1.loc[i][\"Account Number\"] != 1:\n",
    "                print(\"Not 1 Found\")\n",
    "\n",
    "        self.dfs = [self.df1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a26455",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Reorder Time Series Observations (Rows) in Chronological Order for each Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47578714",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class chronological2(DataProcess):\n",
    "    def __init__(self, data_path, excel_or_csv=\"\"):\n",
    "        super().__init__(data_path=data_path, excel_or_csv=excel_or_csv)\n",
    "\n",
    "    def process(self, sort_by=\"bse_ym\"):\n",
    "        self.df_output = pd.DataFrame()\n",
    "        current_acc = \"\"\n",
    "        for i in range(self.df_len):\n",
    "            print(i)\n",
    "            if current_acc == \"\":\n",
    "                current_acc = self.df.loc[i][\"act_no\"]\n",
    "                indicies = [0]\n",
    "            elif current_acc == self.df.loc[i][\"act_no\"]:\n",
    "                indicies.append(i)\n",
    "            elif current_acc != self.df.loc[i][\"act_no\"]:\n",
    "\n",
    "                self.df1 = pd.DataFrame()\n",
    "                self.df1 = self.df.iloc[indicies, :]\n",
    "                self.df1 = self.df1.sort_values(by=sort_by, ascending=True)\n",
    "                self.df_output = self.df_output.append(self.df1, ignore_index=True)\n",
    "\n",
    "                current_acc = self.df.loc[i][\"act_no\"]\n",
    "                indicies = [i]\n",
    "\n",
    "            if i == self.df_len - 1:\n",
    "                self.df1 = pd.DataFrame()\n",
    "                self.df1 = self.df.iloc[indicies, :]\n",
    "                self.df1 = self.df1.sort_values(by=sort_by, ascending=True)\n",
    "                self.df_output = self.df_output.append(self.df1, ignore_index=True)\n",
    "\n",
    "        self.dfs = [self.df_output]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44396917",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Map Y/N to 1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b61ae24",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class yesNo(DataProcess):\n",
    "    def __init__(self, data_path, excel_or_csv=\"\"):\n",
    "        super().__init__(data_path=data_path, excel_or_csv=excel_or_csv)\n",
    "\n",
    "    def process(self, columns=[]):\n",
    "        for column in columns:\n",
    "            for i in range(self.df_len):\n",
    "                print(i)\n",
    "                if self.df.loc[i][column] == \"Y\":\n",
    "                    self.df.at[i, column] = 1\n",
    "                elif self.df.loc[i][column] == \"N\":\n",
    "                    self.df.at[i, column] = 0\n",
    "\n",
    "        self.dfs = [self.df]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21263b3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Check Number of Unique Currencies in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79806a68",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class uniqueFX(DataProcess):\n",
    "    def __init__(self, data_path, data_path1, excel_or_csv=\"\"):\n",
    "        super().__init__(data_path=data_path, data_path1=data_path1, excel_or_csv=excel_or_csv)\n",
    "\n",
    "    def process(self, column=\"\"):\n",
    "        unique_fx = []\n",
    "\n",
    "        for i in range(self.df_len):\n",
    "            if self.df.loc[i][column] not in unique_fx:\n",
    "                unique_fx.append(self.df.loc[i][column])\n",
    "\n",
    "        for i in range(len(self.df1)):\n",
    "            if self.df1.loc[i][column] not in unique_fx:\n",
    "                unique_fx.append(self.df1.loc[i][column])\n",
    "\n",
    "        for fx in unique_fx:\n",
    "            print(fx)\n",
    "\n",
    "        self.dfs = [self.df]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec081f8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Rename Customers and Accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c3dead",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class renameCusAcc(DataProcess):\n",
    "    def __init__(self, data_path, excel_or_csv=\"\"):\n",
    "        super().__init__(data_path=data_path, excel_or_csv=excel_or_csv)\n",
    "\n",
    "    def process(self):\n",
    "        current_cus = \"\"\n",
    "        current_acc = \"\"\n",
    "        j = 0\n",
    "        acc_dict = {}\n",
    "\n",
    "        for i in range(self.df_len):\n",
    "            print(i)\n",
    "            if current_cus == \"\" and current_acc == \"\":\n",
    "                current_cus = self.df.loc[i][\"cus_no\"]\n",
    "                current_acc = self.df.loc[i][\"act_no\"]\n",
    "                acc_dict[self.df.loc[i][\"act_no\"]] = \"acc_\" + str(j)\n",
    "\n",
    "                self.df.at[i, \"cus_no\"] = \"cus_\" + str(j)\n",
    "                self.df.at[i, \"act_no\"] = \"acc_\" + str(j)\n",
    "            elif current_cus == self.df.loc[i][\"cus_no\"] and current_acc == self.df.loc[i][\"act_no\"]:\n",
    "                self.df.at[i, \"cus_no\"] = \"cus_\" + str(j)\n",
    "                self.df.at[i, \"act_no\"] = \"acc_\" + str(j)\n",
    "            elif current_cus != self.df.loc[i][\"cus_no\"] and current_acc != self.df.loc[i][\"act_no\"]:\n",
    "                current_cus = self.df.loc[i][\"cus_no\"]\n",
    "                current_acc = self.df.loc[i][\"act_no\"]\n",
    "\n",
    "                j += 1\n",
    "\n",
    "                acc_dict[self.df.loc[i][\"act_no\"]] = \"acc_\" + str(j)\n",
    "\n",
    "                self.df.at[i, \"cus_no\"] = \"cus_\" + str(j)\n",
    "                self.df.at[i, \"act_no\"] = \"acc_\" + str(j)\n",
    "\n",
    "        self.dfs = [self.df]\n",
    "\n",
    "        return acc_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633b9723",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Rename Accounts\n",
    "#### Drop Customers Since Each Account Observation Maps to a Unique Customer Observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb806bd6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class renameCusAcc1(DataProcess):\n",
    "    def __init__(self, data_path, excel_or_csv=\"\"):\n",
    "        super().__init__(data_path=data_path, excel_or_csv=excel_or_csv)\n",
    "\n",
    "    def process(self, acc_dict={}):\n",
    "        for i in range(self.df_len):\n",
    "            print(i)\n",
    "            self.df.at[i, \"act_no\"] = acc_dict[self.df.loc[i, \"act_no\"]]\n",
    "\n",
    "        self.dfs = [self.df]\n",
    "\n",
    "\n",
    "class renameCusAcc2(DataProcess):\n",
    "    def __init__(self, data_path, excel_or_csv=\"\"):\n",
    "        super().__init__(data_path=data_path, excel_or_csv=excel_or_csv)\n",
    "\n",
    "    def process(self, acc_dict={}):\n",
    "        self.df.drop(\"cus_no\", axis=1, inplace=True)\n",
    "\n",
    "        for i in range(self.df_len):\n",
    "            print(i)\n",
    "            self.df.at[i, \"act_no\"] = acc_dict[self.df.loc[i, \"act_no\"]]\n",
    "\n",
    "        self.dfs = [self.df]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38203f9e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### One Hot Encode FX "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2f1c22",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class oneHotFX(DataProcess):\n",
    "    def __init__(self, data_path, excel_or_csv=\"\"):\n",
    "        super().__init__(data_path=data_path, excel_or_csv=excel_or_csv)\n",
    "\n",
    "    def process(self):\n",
    "        self.df1 = pd.get_dummies(self.df.cur_cd, prefix='FX')\n",
    "        self.df = pd.concat([self.df, self.df1], axis=1)\n",
    "\n",
    "        self.df.drop(\"cur_cd\", axis=1, inplace=True)\n",
    "\n",
    "        self.dfs = [self.df]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877503e6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Order Accounts Ascending for All Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffe6377",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class accToNum(DataProcess):\n",
    "    def __init__(self, data_path, excel_or_csv=\"\"):\n",
    "        super().__init__(data_path=data_path, excel_or_csv=excel_or_csv)\n",
    "\n",
    "    def process(self, drop=\"\"):\n",
    "\n",
    "        if drop != \"\":\n",
    "            self.df = self.df.drop([drop], axis=1)\n",
    "\n",
    "        for i in range(self.df_len):\n",
    "            print(i)\n",
    "            self.df.at[i, \"act_no\"] = int(self.df.loc[i][\"act_no\"][4:])\n",
    "\n",
    "        self.dfs = [self.df]\n",
    "\n",
    "\n",
    "class sortByAcc(DataProcess):\n",
    "    def __init__(self, data_path, excel_or_csv=\"\"):\n",
    "        super().__init__(data_path=data_path, excel_or_csv=excel_or_csv)\n",
    "\n",
    "    def process(self):\n",
    "        self.df = self.df.sort_values(by=\"act_no\", ascending=True, kind=\"mergesort\")  # mergesort is stable\n",
    "\n",
    "        self.dfs = [self.df]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012a9f26",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Remove Unnecessary Index Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ccc234",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class dropUnnamed(DataProcess):\n",
    "    def __init__(self, data_path, excel_or_csv=\"\"):\n",
    "        super().__init__(data_path=data_path, excel_or_csv=excel_or_csv)\n",
    "\n",
    "    def process(self):\n",
    "\n",
    "        for column in self.df:\n",
    "            if \"Unnamed\" in column:\n",
    "                self.df = self.df.drop([column], axis=1)\n",
    "\n",
    "        self.dfs = [self.df]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800b9b4c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Aggregate Monthly Access Count to Representative Integer\n",
    "#### e.g. “110000” → “2”, “111111” → “6”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b537b3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class monthlyAccessCount(DataProcess):\n",
    "    def __init__(self, data_path, excel_or_csv=\"\"):\n",
    "        super().__init__(data_path=data_path, excel_or_csv=excel_or_csv)\n",
    "\n",
    "    def process(self):\n",
    "        for i in range(self.df_len):\n",
    "            print(i)\n",
    "            arr = list(str(self.df.loc[i][\"mts_mm_access_type\"]))\n",
    "\n",
    "            sum = 0\n",
    "            for j in arr:\n",
    "                sum += int(j)\n",
    "\n",
    "            self.df.at[i, \"mts_mm_access_type\"] = sum\n",
    "\n",
    "        self.dfs = [self.df]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea82d73",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### One Hot Encode Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc57abc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class oneHotSex(DataProcess):\n",
    "    def __init__(self, data_path, excel_or_csv=\"\"):\n",
    "        super().__init__(data_path=data_path, excel_or_csv=excel_or_csv)\n",
    "\n",
    "    def process(self):\n",
    "        pd.set_option(\"display.max_columns\", None)\n",
    "        for i in range(self.df_len):\n",
    "\n",
    "            if self.df.loc[i][\"sex_dit_cd\"] == 1:\n",
    "                self.df.at[i, \"sex_dit_cd\"] = \"M\"\n",
    "            elif self.df.loc[i][\"sex_dit_cd\"] == 2:\n",
    "                self.df.at[i, \"sex_dit_cd\"] = \"F\"\n",
    "            elif self.df.loc[i][\"sex_dit_cd\"] == 99:\n",
    "                self.df.at[i, \"sex_dit_cd\"] = \"NA\"\n",
    "\n",
    "        self.df1 = pd.get_dummies(self.df.sex_dit_cd, prefix='SEX')\n",
    "        self.df = pd.concat([self.df, self.df1], axis=1)\n",
    "\n",
    "        self.df.drop(\"sex_dit_cd\", axis=1, inplace=True)\n",
    "\n",
    "        self.dfs = [self.df]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774baaec",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Set Age = 99 to \"NA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16d082f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class processAge(DataProcess):\n",
    "    def __init__(self, data_path, excel_or_csv=\"\"):\n",
    "        super().__init__(data_path=data_path, excel_or_csv=excel_or_csv)\n",
    "\n",
    "    def process(self):\n",
    "        for i in range(self.df_len):\n",
    "            if self.df.loc[i][\"cus_age_stn_cd\"] == 99:\n",
    "                self.df.at[i, \"cus_age_stn_cd\"] = \"NA\"\n",
    "\n",
    "        self.dfs = [self.df]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0235ca",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Regroup Main Traded Security to Proxy for Customer Risk Profile\n",
    "#### 01 Low Risk = {ONLY_CMA, 신용대출, WRAP, MMW, 발행어음, RP, 신탁_퇴직연금}\n",
    "#### 02 Mid Risk = {ELS_DLS, 펀드, 국내채권, 해외채권}\n",
    "#### 03 High Risk = {국내주식, 해외주식, 선물옵션, 금속}\n",
    "#### NA = {99}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfcae85",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class regroupSecurity(DataProcess):\n",
    "    def __init__(self, data_path, excel_or_csv=\"\"):\n",
    "        super().__init__(data_path=data_path, excel_or_csv=excel_or_csv)\n",
    "\n",
    "    def process(self):\n",
    "        low_risk = [1, 15, 14, 13, 12, 11, 10]\n",
    "        mid_risk = [9, 8, 7, 6]\n",
    "        high_risk = [5, 4, 3, 2]\n",
    "        na = [99]\n",
    "\n",
    "        for i in range(self.df_len):\n",
    "            print(i)\n",
    "            if self.df.loc[i][\"mrz_pdt_tp_sgm_cd\"] in low_risk:\n",
    "                self.df.at[i, \"mrz_pdt_tp_sgm_cd\"] = 1\n",
    "            elif self.df.loc[i][\"mrz_pdt_tp_sgm_cd\"] in mid_risk:\n",
    "                self.df.at[i, \"mrz_pdt_tp_sgm_cd\"] = 2\n",
    "            elif self.df.loc[i][\"mrz_pdt_tp_sgm_cd\"] in high_risk:\n",
    "                self.df.at[i, \"mrz_pdt_tp_sgm_cd\"] = 3\n",
    "            elif self.df.loc[i][\"mrz_pdt_tp_sgm_cd\"] in na:\n",
    "                self.df.at[i, \"mrz_pdt_tp_sgm_cd\"] = \"NA\"\n",
    "\n",
    "        self.dfs = [self.df]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d44761c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Set Lifestage = \"99\" to \"NA\"\n",
    "#### One Hot Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0bde63",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class lifestageProcess(DataProcess):\n",
    "    def __init__(self, data_path, excel_or_csv=\"\"):\n",
    "        super().__init__(data_path=data_path, excel_or_csv=excel_or_csv)\n",
    "\n",
    "    def process(self):\n",
    "        for i in range(self.df_len):\n",
    "            if self.df.loc[i][\"lsg_sgm_cd\"] == 99:\n",
    "                self.df.at[i, \"lsg_sgm_cd\"] = \"NA\"\n",
    "\n",
    "        self.df1 = pd.get_dummies(self.df.lsg_sgm_cd, prefix='LIFESTAGE')\n",
    "        self.df = pd.concat([self.df, self.df1], axis=1)\n",
    "\n",
    "        self.df.drop(\"lsg_sgm_cd\", axis=1, inplace=True)\n",
    "\n",
    "        self.dfs = [self.df]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7aae439",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Reverse Numerical Labeling for Intuitive Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1cef57",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class customerLvlProcess(DataProcess):\n",
    "    def __init__(self, data_path, excel_or_csv=\"\"):\n",
    "        super().__init__(data_path=data_path, excel_or_csv=excel_or_csv)\n",
    "\n",
    "    def process(self):\n",
    "        dictionary = {1: 7,\n",
    "                      2: 6,\n",
    "                      3: 5,\n",
    "                      4: 4,\n",
    "                      5: 3,\n",
    "                      9: 2,\n",
    "                      99: 1}\n",
    "\n",
    "        for i in range(self.df_len):\n",
    "            self.df.at[i, \"tco_cus_grd_cd\"] = dictionary[self.df.loc[i][\"tco_cus_grd_cd\"]]\n",
    "\n",
    "        self.dfs = [self.df]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1256eb",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Set Total Investing Duration = \"99\" to \"NA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c148f437",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class totalDurationInvestingProcess(DataProcess):\n",
    "    def __init__(self, data_path, excel_or_csv=\"\"):\n",
    "        super().__init__(data_path=data_path, excel_or_csv=excel_or_csv)\n",
    "\n",
    "    def process(self):\n",
    "        for i in range(self.df_len):\n",
    "            if self.df.loc[i][\"tot_ivs_te_sgm_cd\"] == 99:\n",
    "                self.df.at[i, \"tot_ivs_te_sgm_cd\"] = \"NA\"\n",
    "\n",
    "        self.dfs = [self.df]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9527f84",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Set Holdings Type = \"99\" to \"NA\"\n",
    "#### One Hot Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41ffda6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class holdingsTypeProcessing(DataProcess):\n",
    "    def __init__(self, data_path, excel_or_csv=\"\"):\n",
    "        super().__init__(data_path=data_path, excel_or_csv=excel_or_csv)\n",
    "\n",
    "    def process(self):\n",
    "        for i in range(self.df_len):\n",
    "            if self.df.loc[i][\"hld_pdt_tp_sgm_cd\"] == 99:\n",
    "                self.df.at[i, \"hld_pdt_tp_sgm_cd\"] = \"NA\"\n",
    "\n",
    "        self.df1 = pd.get_dummies(self.df.hld_pdt_tp_sgm_cd, prefix='HOLDINGS_TYPE')\n",
    "        self.df = pd.concat([self.df, self.df1], axis=1)\n",
    "\n",
    "        self.df.drop(\"hld_pdt_tp_sgm_cd\", axis=1, inplace=True)\n",
    "\n",
    "        self.dfs = [self.df]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b237ab",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Set Loyalty = \"99\" to \"NA\"\n",
    "#### Reverse Numerical Labeling for Intuitive Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94406f2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class loyaltyProcess(DataProcess):\n",
    "    def __init__(self, data_path, excel_or_csv=\"\"):\n",
    "        super().__init__(data_path=data_path, excel_or_csv=excel_or_csv)\n",
    "\n",
    "    def process(self):\n",
    "\n",
    "        dictionary = {1: 6,\n",
    "                      2: 5,\n",
    "                      3: 4,\n",
    "                      4: 3,\n",
    "                      5: 2,\n",
    "                      6: 1}\n",
    "\n",
    "        for i in range(self.df_len):\n",
    "            print(i)\n",
    "            if self.df.loc[i][\"loy_sgm_cd\"] == 99:\n",
    "                self.df.at[i, \"loy_sgm_cd\"] = \"NA\"\n",
    "            elif self.df.loc[i][\"loy_sgm_cd\"] in dictionary:\n",
    "                self.df.at[i, \"loy_sgm_cd\"] = dictionary[self.df.loc[i][\"loy_sgm_cd\"]]\n",
    "            else:\n",
    "                self.df.at[i, \"loy_sgm_cd\"] = \"NA\"\n",
    "\n",
    "        self.dfs = [self.df]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2120b8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Set Main Market = \"99\" to \"NA\"\n",
    "#### Regroup Main Market to Proxy for Customer Sophistication\n",
    "#### 01 Low Sophistication = {비매매}\n",
    "#### 02 Mid Sophistication = {거래소, 코스닥, 외화주식}\n",
    "#### 03 High Sophistication = {혼합}\n",
    "#### 04 Very High Sophistication = {ELW}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c64dc0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class mainMarketProcess(DataProcess):\n",
    "    def __init__(self, data_path, excel_or_csv=\"\"):\n",
    "        super().__init__(data_path=data_path, excel_or_csv=excel_or_csv)\n",
    "\n",
    "    def process(self):\n",
    "        low_soph = [6]\n",
    "        mid_soph = [1, 2, 4]\n",
    "        high_soph = [5]\n",
    "        vhigh_soph = [3]\n",
    "\n",
    "        for i in range(self.df_len):\n",
    "            if self.df.loc[i][\"mrz_mkt_dit_cd\"] == 99:\n",
    "                self.df.at[i, \"mrz_mkt_dit_cd\"] = \"NA\"\n",
    "            elif self.df.loc[i][\"mrz_mkt_dit_cd\"] in low_soph:\n",
    "                self.df.at[i, \"mrz_mkt_dit_cd\"] = 1\n",
    "            elif self.df.loc[i][\"mrz_mkt_dit_cd\"] in mid_soph:\n",
    "                self.df.at[i, \"mrz_mkt_dit_cd\"] = 2\n",
    "            elif self.df.loc[i][\"mrz_mkt_dit_cd\"] in high_soph:\n",
    "                self.df.at[i, \"mrz_mkt_dit_cd\"] = 3\n",
    "            elif self.df.loc[i][\"mrz_mkt_dit_cd\"] in vhigh_soph:\n",
    "                self.df.at[i, \"mrz_mkt_dit_cd\"] = 4\n",
    "\n",
    "        self.dfs = [self.df]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8799b88d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Set Main Sector = \"99\" to \"NA\"\n",
    "#### One Hot Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bbedbb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class mainSectorProcess(DataProcess):\n",
    "    def __init__(self, data_path, excel_or_csv=\"\"):\n",
    "        super().__init__(data_path=data_path, excel_or_csv=excel_or_csv)\n",
    "\n",
    "    def process(self):\n",
    "\n",
    "        for i in range(self.df_len):\n",
    "            if self.df.loc[i][\"mrz_btp_dit_cd\"] == 99:\n",
    "                self.df.at[i, \"mrz_btp_dit_cd\"] = \"NA\"\n",
    "\n",
    "        self.df1 = pd.get_dummies(self.df.mrz_btp_dit_cd, prefix='SECTOR')\n",
    "        self.df = pd.concat([self.df, self.df1], axis=1)\n",
    "\n",
    "        self.df.drop(\"mrz_btp_dit_cd\", axis=1, inplace=True)\n",
    "\n",
    "        self.dfs = [self.df]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29d9304",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Set Net Worth = \"99\" to \"NA\"\n",
    "#### Reverse Numerical Labeling for Intuitive Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516f488b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class netWorthProcess(DataProcess):\n",
    "    def __init__(self, data_path, excel_or_csv=\"\"):\n",
    "        super().__init__(data_path=data_path, excel_or_csv=excel_or_csv)\n",
    "\n",
    "    def process(self):\n",
    "\n",
    "        dictionary = {1: 5,\n",
    "                      2: 4,\n",
    "                      3: 3,\n",
    "                      4: 2,\n",
    "                      5: 1}\n",
    "\n",
    "        for i in range(self.df_len):\n",
    "            if self.df.loc[i][\"aet_bse_stk_trd_tp_cd\"] == 99 or self.df.loc[i][\"aet_bse_stk_trd_tp_cd\"] == \"_\":\n",
    "                self.df.at[i, \"aet_bse_stk_trd_tp_cd\"] = \"NA\"\n",
    "            elif self.df.loc[i][\"aet_bse_stk_trd_tp_cd\"] in dictionary:\n",
    "                self.df.at[i, \"aet_bse_stk_trd_tp_cd\"] = dictionary[self.df.loc[i][\"aet_bse_stk_trd_tp_cd\"]]\n",
    "            else:\n",
    "                self.df.at[i, \"aet_bse_stk_trd_tp_cd\"] = \"NA\"\n",
    "\n",
    "        self.dfs = [self.df]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c150dfba",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Set Trade Frequency = \"99\" to \"NA\"\n",
    "#### Reverse Numerical Labeling for Intuitive Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e1c74a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class tradeFrequencyProcess(DataProcess):\n",
    "    def __init__(self, data_path, excel_or_csv=\"\"):\n",
    "        super().__init__(data_path=data_path, excel_or_csv=excel_or_csv)\n",
    "\n",
    "    def process(self):\n",
    "\n",
    "        dictionary = {1: 11,\n",
    "                      2: 10,\n",
    "                      3: 9,\n",
    "                      4: 8,\n",
    "                      5: 7,\n",
    "                      6: 6,\n",
    "                      7: 5,\n",
    "                      8: 4,\n",
    "                      9: 3,\n",
    "                      10: 2,\n",
    "                      11: 1}\n",
    "\n",
    "        for i in range(self.df_len):\n",
    "            if self.df.loc[i][\"bas_stk_trd_tp_cd\"] == 99:\n",
    "                self.df.at[i, \"bas_stk_trd_tp_cd\"] = \"NA\"\n",
    "            if can_convert_to_int(self.df.loc[i][\"bas_stk_trd_tp_cd\"]):\n",
    "                self.df.at[i, \"bas_stk_trd_tp_cd\"] = dictionary[int(self.df.loc[i][\"bas_stk_trd_tp_cd\"])]\n",
    "            else:\n",
    "                self.df.at[i, \"bas_stk_trd_tp_cd\"] = \"NA\"\n",
    "\n",
    "        self.dfs = [self.df]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d46e740",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Drop Column Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb7891d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class dropColumn(DataProcess):\n",
    "    def __init__(self, data_path, excel_or_csv=\"\"):\n",
    "        super().__init__(data_path=data_path, excel_or_csv=excel_or_csv)\n",
    "\n",
    "    def process(self, column_name=\"Unnamed\"):\n",
    "        self.df = self.df.drop([column_name], axis=1)\n",
    "\n",
    "        self.dfs = [self.df]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea74917",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Time Series to Cross Sectional by Taking Max Asset Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f63add4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class maxAssetValue(DataProcess):\n",
    "    def __init__(self, data_path, excel_or_csv=\"\"):\n",
    "        super().__init__(data_path=data_path, excel_or_csv=excel_or_csv)\n",
    "\n",
    "    def process(self):\n",
    "\n",
    "        for i in range(self.df_len):\n",
    "            print(i)\n",
    "            arr = []\n",
    "            for column in ASSET_COLUMNS:\n",
    "                if can_convert_to_int(self.df.loc[i][column]):\n",
    "                    arr.append(self.df.loc[i][column])\n",
    "\n",
    "            self.df.at[i, \"MAX_ASSET_VALUE\"] = max(arr)\n",
    "\n",
    "        for column in ASSET_COLUMNS:\n",
    "            self.df = self.df.drop([column], axis=1)\n",
    "\n",
    "        self.dfs = [self.df]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4e40d9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Derive Account Lifespan from Account Open Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e31006",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class accLifespan(DataProcess):\n",
    "    def __init__(self, data_path, excel_or_csv=\"\"):\n",
    "        super().__init__(data_path=data_path, excel_or_csv=excel_or_csv)\n",
    "\n",
    "    def process(self):\n",
    "        today = date.today()\n",
    "\n",
    "        for i in range(self.df_len):\n",
    "            year = int(str(self.df.loc[i][\"fst_act_opn_dt\"])[0:4])\n",
    "            month = int(str(self.df.loc[i][\"fst_act_opn_dt\"])[4:6])\n",
    "            day = int(str(self.df.loc[i][\"fst_act_opn_dt\"])[6:8])\n",
    "\n",
    "            dateObject = date(year, month, day)\n",
    "\n",
    "            self.df.at[i, \"fst_act_opn_dt\"] = (today - dateObject).days\n",
    "\n",
    "        self.dfs = [self.df]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44dbcb2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Time Series to Cross Sectional by taking the Mode, for Customer Information CSV File\n",
    "#### cross_sectional = mode(time_series): \n",
    "\n",
    "#### {act_no 계좌번호(가명화), \n",
    "#### mts_mm_access_type MTS월단위접속패턴,\n",
    "#### sex_dit_cd 성별 (ONE HOT ENCODED),\n",
    "#### cus_age_stn_cd 연령대,\n",
    "#### pft_amt_stn_cd 수익금액구간코드,\n",
    "#### fst_act_opn_dt 최초계좌개설일,\n",
    "#### lsg_sgm_cd LIFESTAGE세그먼트코드 (ONE HOT ENCODED),\n",
    "#### tco_cus_grd_cd 고객등급코드,\n",
    "#### tot_ivs_te_sgm_cd 총투자기간세그먼트코드,\n",
    "#### hld_pdt_tp_sgm_cd 보유상품유형세그먼트코드 (ONE HOT ENCODED),\n",
    "#### loy_sgm_cd 충성도세그먼트코드,\n",
    "#### mrz_btp_dit_cd 주거래업종구분코드 (ONE HOT ENCODED)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9952c3f6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class infoToCrossSectMode(DataProcess):\n",
    "    def __init__(self, data_path, excel_or_csv=\"\"):\n",
    "        super().__init__(data_path=data_path, excel_or_csv=excel_or_csv)\n",
    "\n",
    "    def process(self):\n",
    "\n",
    "        self.df1 = pd.DataFrame()\n",
    "\n",
    "        for column_name in MODE_COLUMNS:\n",
    "            current_acc = \"\"\n",
    "            k = 0\n",
    "            memory = []\n",
    "\n",
    "            for i in range(self.df_len):\n",
    "                print(column_name, i)\n",
    "\n",
    "                if current_acc == \"\":\n",
    "                    current_acc = self.df.loc[i][\"act_no\"]\n",
    "                    memory.append(self.df.loc[i][column_name])\n",
    "                elif self.df.loc[i][\"act_no\"] == current_acc:\n",
    "                    memory.append(self.df.loc[i][column_name])\n",
    "                elif self.df.loc[i][\"act_no\"] != current_acc:\n",
    "\n",
    "                    self.df1.at[k, \"act_no\"] = current_acc\n",
    "                    self.df1.at[k, column_name] = most_frequent(memory)\n",
    "\n",
    "                    current_acc = self.df.loc[i][\"act_no\"]\n",
    "                    memory = [self.df.loc[i][column_name]]\n",
    "\n",
    "                    k += 1\n",
    "\n",
    "                if i == self.df_len - 1:\n",
    "                    self.df1.at[k, \"act_no\"] = current_acc\n",
    "                    self.df1.at[k, column_name] = most_frequent(memory)\n",
    "\n",
    "        self.dfs = [self.df1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac07a10",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Time Series to Cross Sectional by taking Max, for Customer Information CSV File\n",
    "#### cross_sectional = max(time_series):\n",
    " \n",
    "#### {stk_pdt_hld_yn 주식상품보유여부,\n",
    "#### ose_stk_pdt_hld_yn 해외주식상품보유여부,\n",
    "#### mrz_pdt_tp_sgm_cd 주거래상품군,\n",
    "#### mrz_mkt_dit_cd 주거래시장구분코드,\n",
    "#### aet_bse_stk_trd_tp_cd 자산기준주식거래유형코드,\n",
    "#### bas_stk_trd_tp_cd 기본주식거래유형코드}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b0aaff",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class infoToCrossSectMax(DataProcess):\n",
    "    def __init__(self, data_path, excel_or_csv=\"\"):\n",
    "        super().__init__(data_path=data_path, excel_or_csv=excel_or_csv)\n",
    "\n",
    "    def process(self):\n",
    "\n",
    "        self.df1 = pd.DataFrame()\n",
    "\n",
    "        for column_name in MAX_COLUMNS:\n",
    "            current_acc = \"\"\n",
    "            k = 0\n",
    "            memory = []\n",
    "\n",
    "            for i in range(self.df_len):\n",
    "                print(column_name, i)\n",
    "\n",
    "                if current_acc == \"\":\n",
    "                    current_acc = self.df.loc[i][\"act_no\"]\n",
    "                    memory.append(self.df.loc[i][column_name])\n",
    "                elif self.df.loc[i][\"act_no\"] == current_acc:\n",
    "                    memory.append(self.df.loc[i][column_name])\n",
    "                elif self.df.loc[i][\"act_no\"] != current_acc:\n",
    "\n",
    "                    self.df1.at[k, \"act_no\"] = current_acc\n",
    "                    self.df1.at[k, column_name] = max(memory)\n",
    "\n",
    "                    current_acc = self.df.loc[i][\"act_no\"]\n",
    "                    memory = [self.df.loc[i][column_name]]\n",
    "\n",
    "                    k += 1\n",
    "\n",
    "                if i == self.df_len - 1:\n",
    "                    self.df1.at[k, \"act_no\"] = current_acc\n",
    "                    self.df1.at[k, column_name] = most_frequent(memory)\n",
    "\n",
    "        self.dfs = [self.df1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5b7da7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Concatenate Two Data Frames Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7750dbaf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class concatDataframes(DataProcess):\n",
    "    def __init__(self, data_path, data_path1, excel_or_csv=\"\"):\n",
    "        super().__init__(data_path=data_path, data_path1=data_path1, excel_or_csv=excel_or_csv)\n",
    "\n",
    "    def process(self):\n",
    "        self.df = pd.concat([self.df, self.df1], axis=1)\n",
    "\n",
    "        self.dfs = [self.df]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033509db",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Concatenate Two Data Frames Class with Specific Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431e48a0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class concatDataframes2(DataProcess):\n",
    "    def __init__(self, data_path, data_path1, excel_or_csv=\"\"):\n",
    "        super().__init__(data_path=data_path, data_path1=data_path1, excel_or_csv=excel_or_csv)\n",
    "\n",
    "    def process(self, column_name=\"\"):\n",
    "        self.df1 = self.df1[[column_name]]\n",
    "        self.df = pd.concat([self.df, self.df1], axis=1)\n",
    "\n",
    "        self.dfs = [self.df]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbfa10b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Concatenate Two Data Frames Class with Specific Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469bcded",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class concatAll(DataProcess):\n",
    "    def __init__(self, data_path, data_path1, data_path2, excel_or_csv=\"\"):\n",
    "        super().__init__(data_path=data_path, data_path1=data_path1, data_path2=data_path2, excel_or_csv=excel_or_csv)\n",
    "\n",
    "    def process(self, path1_column=\"\", path2_column=\"\", path2_column1=\"\"):\n",
    "        self.df1 = self.df1[[path1_column]]\n",
    "        self.df2 = self.df2[[path2_column, path2_column1]]\n",
    "\n",
    "        self.df = pd.concat([self.df, self.df1, self.df2], axis=1)\n",
    "\n",
    "        self.dfs = [self.df]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f671f7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Remove White Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532eb3d0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class removeWhiteSpace(DataProcess):\n",
    "    def __init__(self, data_path, excel_or_csv=\"\"):\n",
    "        super().__init__(data_path=data_path, excel_or_csv=excel_or_csv)\n",
    "\n",
    "    def process(self, column_name=\"\"):\n",
    "        for i in range(self.df_len):\n",
    "\n",
    "            self.df.at[i, column_name] = str(self.df.loc[i][column_name]).strip()\n",
    "\n",
    "        self.dfs = [self.df]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3613f98f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### ISIN to Ticker using investpy Package\n",
    "#### 외부 데이터 사용:\n",
    "#### \"vol_3m\" Column: Standard Deviation of Daily Log Normal Price Returns for Korean Securities with Historic 3 Month Rolling Window Annualized by a Factor of 250. Data Sourced Based on Date: 2020-12-31 to Avoid Look-ahead Bias. Data Sourced from S&P Capital IQ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea270a5b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class getTicker(DataProcess):\n",
    "    def __init__(self, data_path, excel_or_csv=\"\"):\n",
    "        super().__init__(data_path=data_path, excel_or_csv=excel_or_csv)\n",
    "\n",
    "    def process(self):\n",
    "        for i in range(self.df_len):\n",
    "            if self.df.loc[i][\"vol_3m\"] == \"_\":\n",
    "                print(i)\n",
    "                try:\n",
    "                    df = investpy.stocks.search_stocks(by='isin', value=str(self.df.loc[i][\"iem_cd\"]))\n",
    "                    ticker = df.loc[0]['symbol']\n",
    "                except:\n",
    "                    ticker = \"_\"\n",
    "\n",
    "                self.df.at[i, \"new_ticker\"] = ticker\n",
    "            else:\n",
    "                self.df.at[i, \"new_ticker\"] = \"__\"\n",
    "\n",
    "        self.dfs = [self.df]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ebc627",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Drop Uneccesary Columns in Account CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c00229",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class accountDrop(DataProcess):\n",
    "    def __init__(self, data_path, excel_or_csv=\"\"):\n",
    "        super().__init__(data_path=data_path, excel_or_csv=excel_or_csv)\n",
    "\n",
    "    def process(self):\n",
    "        for column in DROP_COLUMNS:\n",
    "            self.df = self.df.drop([column], axis=1)\n",
    "\n",
    "        self.dfs = [self.df]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513d72ea",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Create \"final_vol_3m\" Column Which Combines \"vol_3m\" and \"new_vol_3m\"\n",
    "#### 외부 데이터 사용:\n",
    "#### \"new_vol_3m\" Column: Standard Deviation of Daily Log Normal Price Returns for Overseas Securities with Historic 3 Month Rolling Window Annualized by a Factor of 250. Data Sourced Based on Date: 2020-12-31 to Avoid Look-ahead Bias. Data Sourced from S&P Capital IQ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a3bebc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class finalVol3M(DataProcess):\n",
    "    def __init__(self, data_path, excel_or_csv=\"\"):\n",
    "        super().__init__(data_path=data_path, excel_or_csv=excel_or_csv)\n",
    "\n",
    "    def process(self):\n",
    "        for i in range(self.df_len):\n",
    "            print(i)\n",
    "            if isfloat(self.df.loc[i][\"vol_3m\"]):\n",
    "                self.df.at[i, \"final_vol_3m\"] = self.df.loc[i][\"vol_3m\"]\n",
    "            elif isfloat(self.df.loc[i][\"new_vol_3m\"]):\n",
    "                self.df.at[i, \"final_vol_3m\"] = self.df.loc[i][\"new_vol_3m\"]\n",
    "            else:\n",
    "                self.df.at[i, \"final_vol_3m\"] = \"_\"\n",
    "\n",
    "        self.dfs = [self.df]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e850c90a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Derive Value Weighted Portfolio Volatility to Proxy for Customer Risk Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e76043",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class valueWeightedVolatility(DataProcess):\n",
    "    def __init__(self, data_path, excel_or_csv=\"\"):\n",
    "        super().__init__(data_path=data_path, excel_or_csv=excel_or_csv)\n",
    "\n",
    "    def process(self):\n",
    "        self.df1 = pd.DataFrame()\n",
    "        value_memory = []\n",
    "        volatility_memory = []\n",
    "\n",
    "        current_acc = \"\"\n",
    "        k = 0\n",
    "\n",
    "        for i in range(self.df_len):\n",
    "            print(i)\n",
    "            if current_acc == \"\":\n",
    "                current_acc = self.df.loc[i][\"act_no\"]\n",
    "                if can_convert_to_int(self.df.loc[i][\"stl_bse_now_eal_amt\"]) and isfloat(self.df.loc[i][\"final_vol_3m\"]):\n",
    "                    value_memory.append(self.df.loc[i][\"stl_bse_now_eal_amt\"])\n",
    "                    volatility_memory.append(self.df.loc[i][\"final_vol_3m\"])\n",
    "            elif self.df.loc[i][\"act_no\"] == current_acc:\n",
    "                if can_convert_to_int(self.df.loc[i][\"stl_bse_now_eal_amt\"]) and isfloat(self.df.loc[i][\"final_vol_3m\"]):\n",
    "                    value_memory.append(self.df.loc[i][\"stl_bse_now_eal_amt\"])\n",
    "                    volatility_memory.append(self.df.loc[i][\"final_vol_3m\"])\n",
    "            elif self.df.loc[i][\"act_no\"] != current_acc:\n",
    "                self.df1.at[k, \"act_no\"] = current_acc\n",
    "\n",
    "                if len(value_memory) > 0 and len(volatility_memory) > 0:\n",
    "                    weight_list = []\n",
    "                    sum = np.sum(value_memory)\n",
    "\n",
    "                    for value in value_memory:\n",
    "                        weight_list.append(value/sum)\n",
    "\n",
    "                    value_weighted_volatility = 0\n",
    "                    for j in range(len(volatility_memory)):\n",
    "                        value_weighted_volatility += float(volatility_memory[j]) * float(weight_list[j])\n",
    "\n",
    "                    self.df1.at[k, \"value_weighted_volatility\"] = value_weighted_volatility\n",
    "                else:\n",
    "                    self.df1.at[k, \"value_weighted_volatility\"] = \"_\"\n",
    "\n",
    "                value_memory = []\n",
    "                volatility_memory = []\n",
    "                if can_convert_to_int(self.df.loc[i][\"stl_bse_now_eal_amt\"]) and isfloat(self.df.loc[i][\"final_vol_3m\"]):\n",
    "                    value_memory.append(self.df.loc[i][\"stl_bse_now_eal_amt\"])\n",
    "                    volatility_memory.append(self.df.loc[i][\"final_vol_3m\"])\n",
    "\n",
    "                current_acc = self.df.loc[i][\"act_no\"]\n",
    "                k += 1\n",
    "\n",
    "            if i == self.df_len - 1:\n",
    "                self.df1.at[k, \"act_no\"] = current_acc\n",
    "                if can_convert_to_int(self.df.loc[i][\"stl_bse_now_eal_amt\"]) and isfloat(self.df.loc[i][\"final_vol_3m\"]):\n",
    "                    value_memory.append(self.df.loc[i][\"stl_bse_now_eal_amt\"])\n",
    "                    volatility_memory.append(self.df.loc[i][\"final_vol_3m\"])\n",
    "\n",
    "                if len(value_memory) > 0 and len(volatility_memory) > 0:\n",
    "                    weight_list = []\n",
    "                    sum = np.sum(value_memory)\n",
    "\n",
    "                    for value in value_memory:\n",
    "                        weight_list.append(value / sum)\n",
    "\n",
    "                    value_weighted_volatility = 0\n",
    "                    for j in range(len(volatility_memory)):\n",
    "                        value_weighted_volatility += float(volatility_memory[j]) * float(weight_list[j])\n",
    "\n",
    "                    self.df1.at[k, \"value_weighted_volatility\"] = value_weighted_volatility\n",
    "                else:\n",
    "                    self.df1.at[k, \"value_weighted_volatility\"] = \"_\"\n",
    "\n",
    "        self.dfs = [self.df1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229e3650",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Aggregate Leverage for Each Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c68b8d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class aggregateLeverage(DataProcess):\n",
    "    def __init__(self, data_path, excel_or_csv=\"\"):\n",
    "        super().__init__(data_path=data_path, excel_or_csv=excel_or_csv)\n",
    "\n",
    "    def process(self):\n",
    "        self.df1 = pd.DataFrame()\n",
    "        leverage = 0\n",
    "\n",
    "        current_acc = \"\"\n",
    "        k = 0\n",
    "\n",
    "        for i in range(self.df_len):\n",
    "            print(i)\n",
    "            if current_acc == \"\":\n",
    "                leverage += float(self.df.loc[i][\"lon_amt\"])\n",
    "                current_acc = self.df.loc[i][\"act_no\"]\n",
    "            elif self.df.loc[i][\"act_no\"] == current_acc:\n",
    "                leverage += float(self.df.loc[i][\"lon_amt\"])\n",
    "            elif self.df.loc[i][\"act_no\"] != current_acc:\n",
    "                self.df1.at[k, \"act_no\"] = current_acc\n",
    "                self.df1.at[k, \"leverage\"] = leverage\n",
    "\n",
    "                leverage = 0\n",
    "                current_acc = self.df.loc[i][\"act_no\"]\n",
    "                k += 1\n",
    "\n",
    "            if i == self.df_len - 1:\n",
    "                self.df1.at[k, \"act_no\"] = current_acc\n",
    "                self.df1.at[k, \"leverage\"] = leverage\n",
    "\n",
    "        self.dfs = [self.df1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92abacf0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Drop Observations (Rows) with Missing Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c10c53",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class dropMissingData(DataProcess):\n",
    "    def __init__(self, data_path, excel_or_csv=\"\"):\n",
    "        super().__init__(data_path=data_path, excel_or_csv=excel_or_csv)\n",
    "\n",
    "    def process(self):\n",
    "        for i in range(self.df_len):\n",
    "            print(i)\n",
    "            if self.df.loc[i][\"cus_age_stn_cd\"] == \"_\":\n",
    "                self.df.drop(index=i, axis=0, inplace=True)\n",
    "                continue\n",
    "            if self.df.loc[i][\"tot_ivs_te_sgm_cd\"] == \"_\":\n",
    "                self.df.drop(index=i, axis=0, inplace=True)\n",
    "                continue\n",
    "            if self.df.loc[i][\"value_weighted_volatility\"] == \"_\" or self.df.loc[i][\"value_weighted_volatility\"] == 0 or self.df.loc[i][\"value_weighted_volatility\"] == \"0\":\n",
    "                self.df.drop(index=i, axis=0, inplace=True)\n",
    "                continue\n",
    "\n",
    "        self.df.reset_index(inplace=True)\n",
    "\n",
    "        self.dfs = [self.df]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dbde5f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Process Observations (Rows) with Missing Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718d0134",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class processMissingData(DataProcess):\n",
    "    def __init__(self, data_path, excel_or_csv=\"\"):\n",
    "        super().__init__(data_path=data_path, excel_or_csv=excel_or_csv)\n",
    "\n",
    "    def process(self):\n",
    "        for i in range(self.df_len):\n",
    "            print(i)\n",
    "            if self.df.loc[i][\"loy_sgm_cd\"] == \"_\":\n",
    "                self.df.at[i, \"loy_sgm_cd\"] = 1\n",
    "            if self.df.loc[i][\"mrz_pdt_tp_sgm_cd\"] == \"_\":\n",
    "                self.df.at[i, \"mrz_pdt_tp_sgm_cd\"] = 1\n",
    "            if self.df.loc[i][\"mrz_mkt_dit_cd\"] == \"_\":\n",
    "                self.df.at[i, \"mrz_mkt_dit_cd\"] = 1\n",
    "            if self.df.loc[i][\"aet_bse_stk_trd_tp_cd\"] == \"_\":\n",
    "                self.df.at[i, \"aet_bse_stk_trd_tp_cd\"] = 1\n",
    "            if self.df.loc[i][\"bas_stk_trd_tp_cd\"] == \"_\":\n",
    "                self.df.at[i, \"bas_stk_trd_tp_cd\"] = 1\n",
    "\n",
    "        self.dfs = [self.df]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94203334",
   "metadata": {},
   "source": [
    "#### One Hot Encode Account Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498a9a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class oneHotAccount(DataProcess):\n",
    "    def __init__(self, data_path, excel_or_csv=\"\"):\n",
    "        super().__init__(data_path=data_path, excel_or_csv=excel_or_csv)\n",
    "\n",
    "    def process(self):\n",
    "        self.df1 = pd.get_dummies(self.df.act_no, prefix='ACCOUNT')\n",
    "        self.df = pd.concat([self.df1, self.df], axis=1)\n",
    "\n",
    "        self.df.drop(\"act_no\", axis=1, inplace=True)\n",
    "\n",
    "        self.dfs = [self.df]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d631f8e",
   "metadata": {},
   "source": [
    "#### MinMax Scale Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edcd0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class minMaxScale(DataProcess):\n",
    "    def __init__(self, data_path, excel_or_csv=\"\"):\n",
    "        super().__init__(data_path=data_path, excel_or_csv=excel_or_csv)\n",
    "\n",
    "    def process(self):\n",
    "        scaler = MinMaxScaler()\n",
    "\n",
    "        columns_list = []\n",
    "        for column in self.df:\n",
    "            columns_list.append(column)\n",
    "\n",
    "        self.df = pd.DataFrame(scaler.fit_transform(self.df), columns=columns_list)\n",
    "\n",
    "        self.dfs = [self.df]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c850ffb4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data Process Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cf209f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "process = customerAccount(data_path=\"cus_info.xlsx\")\n",
    "process.process()\n",
    "process.export([\"cus_acc.csv\"], excel_or_csv=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d295814b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for file_name in RAW_DATA:\n",
    "    process1 = chronological2(data_path=file_name + \".xlsx\")\n",
    "    process1.process(sort_by=\"bse_ym\")\n",
    "    process1.export([file_name + \"_R1.csv\"], excel_or_csv=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9e1d58",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for file_name in RAW_DATA1:\n",
    "    process1 = chronological2(data_path=file_name + \".xlsx\")\n",
    "    process1.process(sort_by=\"orr_dt\")\n",
    "    process1.export([file_name + \"_R1.csv\"], excel_or_csv=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fa78b5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "process2 = yesNo(data_path=\"cus_info_R1\" + \".csv\", excel_or_csv=\"csv\")\n",
    "process2.process(columns=[\"stk_pdt_hld_yn\", \"ose_stk_pdt_hld_yn\"])\n",
    "process2.export([\"cus_info\" + \"_R2.csv\"], excel_or_csv=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965b318b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "process3 = uniqueFX(data_path=\"os_equity_R1.csv\", data_path1=\"cus_account_R1.csv\", excel_or_csv=\"csv\")\n",
    "process3.process(column=\"cur_cd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657f1b54",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "process4 = renameCusAcc(data_path=\"cus_info_R2.csv\", excel_or_csv=\"csv\")\n",
    "acc_dict = process4.process()\n",
    "process4.export([\"cus_info_R3.csv\"], excel_or_csv=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e39b67",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for file_name in DATA_PATH:\n",
    "    process5 = renameCusAcc1(data_path=file_name + \"_R1.csv\", excel_or_csv=\"csv\")\n",
    "    process5.process(acc_dict=acc_dict)\n",
    "    process5.export([file_name + \"_R2.csv\"], excel_or_csv=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957efcda",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "process6 = renameCusAcc2(data_path=\"cus_assets.xlsx\")\n",
    "process6.process(acc_dict=acc_dict)\n",
    "process6.export([\"cus_assets_R1.csv\"], excel_or_csv=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676a7308",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for file_name in DATA_PATH1:\n",
    "    process7 = oneHotFX(data_path=file_name + \"_R2.csv\", excel_or_csv=\"csv\")\n",
    "    process7.process()\n",
    "    process7.export([file_name + \"_R3.csv\"], excel_or_csv=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780463d3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for file_name in DATA_PATH2:\n",
    "    process8 = accToNum(data_path=file_name + \".csv\", excel_or_csv=\"csv\")\n",
    "    process8.process()\n",
    "    process8.export([file_name[:-3] + \"_R4.csv\"], excel_or_csv=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc66bf87",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for file_name in DATA_PATH3:\n",
    "    process9 = sortByAcc(data_path=file_name + \"_R4.csv\", excel_or_csv=\"csv\")\n",
    "    process9.process()  # merge sort is used for stability\n",
    "    process9.export([file_name + \"_R5.csv\"], excel_or_csv=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d39db5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for file_name in DATA_PATH4:\n",
    "    process10 = dropUnnamed(data_path=file_name + \".csv\", excel_or_csv=\"csv\")\n",
    "    process10.process()\n",
    "    process10.export([file_name[:-3] + \"_R6.csv\"], excel_or_csv=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d39397",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "process11 = accToNum(data_path=\"cus_info_R6\" + \".csv\", excel_or_csv=\"csv\")\n",
    "process11.process(drop=\"cus_no\")\n",
    "process11.export([\"cus_info_R7.csv\"], excel_or_csv=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5628f82e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "process12 = monthlyAccessCount(data_path=\"cus_info_R7\" + \".csv\", excel_or_csv=\"csv\")\n",
    "process12.process()\n",
    "process12.export([\"cus_info_R8.csv\"], excel_or_csv=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cc371f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "process13 = oneHotSex(data_path=\"cus_info_R8\" + \".csv\", excel_or_csv=\"csv\")\n",
    "process13.process()\n",
    "process13.export([\"cus_info_R9.csv\"], excel_or_csv=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8968ed",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "process14 = processAge(data_path=\"cus_info_R9\" + \".csv\", excel_or_csv=\"csv\")\n",
    "process14.process()\n",
    "process14.export([\"cus_info_R10.csv\"], excel_or_csv=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46941f6c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "process15 = regroupSecurity(data_path=\"cus_info_R10\" + \".csv\", excel_or_csv=\"csv\")\n",
    "process15.process()\n",
    "process15.export([\"cus_info_R11.csv\"], excel_or_csv=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027bf8af",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "process16 = lifestageProcess(data_path=\"cus_info_R11\" + \".csv\", excel_or_csv=\"csv\")\n",
    "process16.process()\n",
    "process16.export([\"cus_info_R12.csv\"], excel_or_csv=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4d5127",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "process17 = totalDurationInvestingProcess(data_path=\"cus_info_R13\" + \".csv\", excel_or_csv=\"csv\")\n",
    "process17.process()\n",
    "process17.export([\"cus_info_R14.csv\"], excel_or_csv=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498192fc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "process18 = holdingsTypeProcessing(data_path=\"cus_info_R14\" + \".csv\", excel_or_csv=\"csv\")\n",
    "process18.process()\n",
    "process18.export([\"cus_info_R15.csv\"], excel_or_csv=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924a3c05",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "process19 = loyaltyProcess(data_path=\"cus_info_R15\" + \".csv\", excel_or_csv=\"csv\")\n",
    "process19.process()\n",
    "process19.export([\"cus_info_R16.csv\"], excel_or_csv=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f5a89a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "process20 = mainMarketProcess(data_path=\"cus_info_R16\" + \".csv\", excel_or_csv=\"csv\")\n",
    "process20.process()\n",
    "process20.export([\"cus_info_R17.csv\"], excel_or_csv=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb57f294",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "process21 = mainSectorProcess(data_path=\"cus_info_R17\" + \".csv\", excel_or_csv=\"csv\")\n",
    "process21.process()\n",
    "process21.export([\"cus_info_R18.csv\"], excel_or_csv=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df3d5ed",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "process22 = netWorthProcess(data_path=\"cus_info_R18\" + \".csv\", excel_or_csv=\"csv\")\n",
    "process22.process()\n",
    "process22.export([\"cus_info_R19.csv\"], excel_or_csv=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4793aec3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "process23 = tradeFrequencyProcess(data_path=\"cus_info_R19\" + \".csv\", excel_or_csv=\"csv\")\n",
    "process23.process()\n",
    "process23.export([\"cus_info_R20.csv\"], excel_or_csv=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4b2ca8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "process24 = dropColumn(data_path=\"cus_assets_R6.csv\", excel_or_csv=\"csv\")\n",
    "process24.process(column_name=\"mts_mm_access_type\")\n",
    "process24.export([\"cus_assets_R7.csv\"], excel_or_csv=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a996056",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "process25 = dropColumn(data_path=\"cus_account_R6.csv\", excel_or_csv=\"csv\")\n",
    "process25.process(column_name=\"itg_pdt_tp_cd\")\n",
    "process25.export([\"cus_account_R7.csv\"], excel_or_csv=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9fff9b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "process26 = maxAssetValue(data_path=\"cus_assets_R7.csv\", excel_or_csv=\"csv\")\n",
    "process26.process()\n",
    "process26.export([\"cus_assets_R8.csv\"], excel_or_csv=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180d52aa",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "process27 = dropColumn(data_path=\"cus_info_R20.csv\", excel_or_csv=\"csv\")\n",
    "process27.process(column_name=\"cus_aet_stn_cd\")\n",
    "process27.export([\"cus_info_R21.csv\"], excel_or_csv=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0ef676",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "process28 = accLifespan(data_path=\"cus_info_R21.csv\", excel_or_csv=\"csv\")\n",
    "process28.process()\n",
    "process28.export([\"cus_info_R22.csv\"], excel_or_csv=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735b2b98",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "process29 = dropUnnamed(data_path=\"cus_info_R22.csv\", excel_or_csv=\"csv\")\n",
    "process29.process()\n",
    "process29.export([\"cus_info_R23.csv\"], excel_or_csv=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d283ba",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "process30 = infoToCrossSectMode(data_path=\"cus_info_R23.csv\", excel_or_csv=\"csv\")\n",
    "process30.process()\n",
    "process30.export([\"cus_info_R23_mode.csv\"], excel_or_csv=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3222ef",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "process31 = infoToCrossSectMax(data_path=\"cus_info_R23.csv\", excel_or_csv=\"csv\")\n",
    "process31.process()\n",
    "process31.export([\"cus_info_R23_max.csv\"], excel_or_csv=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0585202",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "process32 = concatDataframes(data_path=\"cus_info_R23_mode.csv\", data_path1=\"cus_info_R23_max.csv\", excel_or_csv=\"csv\")\n",
    "process32.process()\n",
    "process32.export([\"cus_info_R24.csv\"], excel_or_csv=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30384364",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "process33 = dropUnnamed(data_path=\"cus_info_R24.csv\", excel_or_csv=\"csv\")\n",
    "process33.process()\n",
    "process33.export([\"cus_info_R25.csv\"], excel_or_csv=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82885fe",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "process34 = dropColumn(data_path=\"cus_account_R7.csv\", excel_or_csv=\"csv\")\n",
    "process34.process(column_name=\"fc_sec_trd_nat_cd\")\n",
    "process34.export([\"cus_account_R8.csv\"], excel_or_csv=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2699e04",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "process35 = removeWhiteSpace(data_path=\"cus_account_R8.csv\", excel_or_csv=\"csv\")\n",
    "process35.process(column_name=\"iem_cd\")\n",
    "process35.export([\"cus_account_R9.csv\"], excel_or_csv=\"csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdc7cce",
   "metadata": {},
   "source": [
    "#### 외부 데이터 사용:\n",
    "#### cus_account_R10.xlsx is cus_account_R9.csv concatenated with new  column \"vol_3m\", where non-available data is \"_\"\n",
    "\n",
    "#### \"vol_3m\" Column: Standard Deviation of Daily Log Normal Price Returns for Korean Securities with Historic 3 Month Rolling Window Annualized by a Factor of 250. Data Sourced Based on Date: 2020-12-31 to Avoid Look-ahead Bias. Data Sourced from S&P Capital IQ.\n",
    "\n",
    "#### \"new_ticker\" Column: Security Ticker Data Sourced from investpy Package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f0e47c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "process36 = getTicker(data_path=\"cus_account_R10.xlsx\")\n",
    "process36.process()\n",
    "process36.export([\"cus_account_R11.csv\"], excel_or_csv=\"csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6481099",
   "metadata": {},
   "source": [
    "#### 외부 데이터 사용:\n",
    "#### cus_account_R12.xlsx is cus_account_R11.csv concatenated with new  column \"new_vol_3m\", where non-available data is \"_\"\n",
    "\n",
    "#### \"new_vol_3m\" Column: Standard Deviation of Daily Log Normal Price Returns for Overseas Securities with Historic 3 Month Rolling Window Annualized by a Factor of 250. Data Sourced Based on Date: 2020-12-31 to Avoid Look-ahead Bias. Data Sourced from S&P Capital IQ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e6a7ed",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "process37 = accountDrop(data_path=\"cus_account_R12.xlsx\")\n",
    "process37.process()\n",
    "process37.export([\"cus_account_R13.csv\"], excel_or_csv=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e887f4ff",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "process38 = dropUnnamed(data_path=\"cus_account_R13.csv\", excel_or_csv=\"csv\")\n",
    "process38.process()\n",
    "process38.export([\"cus_account_R14.csv\"], excel_or_csv=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ead74f6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "process39 = finalVol3M(data_path=\"cus_account_R14.csv\", excel_or_csv=\"csv\")\n",
    "process39.process()\n",
    "process39.export([\"cus_account_R15.csv\"], excel_or_csv=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37414289",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "process40 = valueWeightedVolatility(data_path=\"cus_account_R15.csv\", excel_or_csv=\"csv\")\n",
    "process40.process()\n",
    "process40.export([\"cus_account_R16.csv\"], excel_or_csv=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448ab7b3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "process41 = aggregateLeverage(data_path=\"cus_account_R15.csv\", excel_or_csv=\"csv\")\n",
    "process41.process()\n",
    "process41.export([\"cus_account_R17.csv\"], excel_or_csv=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92907ea",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "process42 = concatDataframes2(data_path=\"cus_account_R16.csv\", data_path1=\"cus_account_R17.csv\", excel_or_csv=\"csv\")\n",
    "process42.process(column_name=\"leverage\")\n",
    "process42.export([\"cus_account_R18.csv\"], excel_or_csv=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8c6e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "process43 = concatAll(data_path=\"cus_info_R25.csv\", data_path1=\"cus_assets_R8.csv\", data_path2=\"cus_account_R18.csv\", excel_or_csv=\"csv\")\n",
    "process43.process(path1_column=\"MAX_ASSET_VALUE\", path2_column=\"value_weighted_volatility\", path2_column1=\"leverage\")\n",
    "process43.export([\"all_R1.csv\"], excel_or_csv=\"csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88969ba5",
   "metadata": {},
   "source": [
    "#### All Empty (NaN) Cells in \"all_R1.csv\" are Replaced With \"_\" on Excel Due to Computational Efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c8e928",
   "metadata": {},
   "outputs": [],
   "source": [
    "process44 = dropColumn(data_path=\"all_R1.csv\", excel_or_csv=\"csv\")\n",
    "process44.process(column_name=\"SEX_NA\")\n",
    "process44.export([\"all_R2.csv\"], excel_or_csv=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1141c665",
   "metadata": {},
   "outputs": [],
   "source": [
    "process45 = dropColumn(data_path=\"all_R2.csv\", excel_or_csv=\"csv\")\n",
    "process45.process(column_name=\"LIFESTAGE_NA\")\n",
    "process45.export([\"all_R3.csv\"], excel_or_csv=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e216def",
   "metadata": {},
   "outputs": [],
   "source": [
    "process46 = dropColumn(data_path=\"all_R3.csv\", excel_or_csv=\"csv\")\n",
    "process46.process(column_name=\"HOLDINGS_TYPE_NA\")\n",
    "process46.export([\"all_R4.csv\"], excel_or_csv=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea53210d",
   "metadata": {},
   "outputs": [],
   "source": [
    "process47 = dropColumn(data_path=\"all_R4.csv\", excel_or_csv=\"csv\")\n",
    "process47.process(column_name=\"act_no.1\")\n",
    "process47.export([\"all_R5.csv\"], excel_or_csv=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e7e081",
   "metadata": {},
   "outputs": [],
   "source": [
    "process48 = dropMissingData(data_path=\"all_R5.csv\", excel_or_csv=\"csv\")\n",
    "process48.process()\n",
    "process48.export([\"all_R6.csv\"], excel_or_csv=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870b2a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "process49 = processMissingData(data_path=\"all_R6.csv\", excel_or_csv=\"csv\")\n",
    "process49.process()\n",
    "process49.export([\"all_R7.csv\"], excel_or_csv=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dba30a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "process50 = dropColumn(data_path=\"all_R7.csv\", excel_or_csv=\"csv\")\n",
    "process50.process(column_name=\"index\")\n",
    "process50.export([\"all_R8.csv\"], excel_or_csv=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46413be",
   "metadata": {},
   "outputs": [],
   "source": [
    "process51 = dropUnnamed(data_path=\"all_R8.csv\", excel_or_csv=\"csv\")\n",
    "process51.process()\n",
    "process51.export([\"all_R9.csv\"], excel_or_csv=\"csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f98b27",
   "metadata": {},
   "source": [
    "### Manually Changed due to Computational Inefficiency\n",
    "#### mts_mm_access_type → access_count\n",
    "#### cus_age_stn_cd → age_group\n",
    "#### pft_amt_stn_cd → income_group\n",
    "#### fst_act_opn_dt → account_age\n",
    "#### tco_cus_grd_cd → nh_level\n",
    "#### tot_ivs_te_sgm_cd → invest_exp_group\n",
    "#### loy_sgm_cd → loyalty\n",
    "#### stk_pdt_hld_yn → kr_stock_yn\n",
    "#### ose_stk_pdt_hld_yn → overseas_stock_yn\n",
    "#### mrz_pdt_tp_sgm_cd → risk_tolerance\n",
    "#### mrz_mkt_dit_cd → sophistication\n",
    "#### aet_bse_stk_trd_tp_cd → stock_wealth_level\n",
    "#### bas_stk_trd_tp_cd → trading_frequency\n",
    "#### MAX_ASSET_VALUE → peak_asset_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cee5c09",
   "metadata": {},
   "source": [
    "### Outlier Removal: R10: Removed Observations (Accounts) in Top 0.1% of Peak Assets and/or Top 0.1%  of Leverage Through Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b150b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "process52 = oneHotAccount(data_path=\"all_R10.csv\", excel_or_csv=\"csv\")\n",
    "process52.process()\n",
    "process52.export([\"all_R11.csv\"], excel_or_csv=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43aa96f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "process53 = minMaxScale(data_path=\"all_R11.csv\", excel_or_csv=\"csv\")\n",
    "process53.process()\n",
    "process53.export([\"all_R12.csv\"], excel_or_csv=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc21315",
   "metadata": {},
   "outputs": [],
   "source": [
    "process54 = dropUnnamed(data_path=\"all_R12.csv\", excel_or_csv=\"csv\")\n",
    "process54.process()\n",
    "process54.export([\"all_R13.csv\"], excel_or_csv=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8f08de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77325d47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40a2b914",
   "metadata": {},
   "source": [
    "# Autoencoder for feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1813479",
   "metadata": {},
   "source": [
    "## Linear Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fec4dbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Autoencoder,self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,16),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(16,32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,output_dim),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "                \n",
    "    def forward(self,x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)  \n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066feaf3",
   "metadata": {},
   "source": [
    "## Convolutional Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "255a3e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "class ConvAutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(ConvAutoEncoder, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.cnn_layer1 = nn.Sequential(\n",
    "                            nn.Conv1d(input_dim, 16, kernel_size=3, stride=1, padding=1),\n",
    "                            nn.ReLU(),\n",
    "                            nn.MaxPool1d(2,2))\n",
    "\n",
    "        self.cnn_layer2 = nn.Sequential(\n",
    "                            nn.Conv1d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "                            nn.ReLU(),\n",
    "                            nn.MaxPool1d(2,2))\n",
    "\n",
    "        # Decoder\n",
    "        self.tran_cnn_layer1 = nn.Sequential(\n",
    "                            nn.ConvTranspose1d(32, 16, kernel_size = 2, stride = 2, padding=0),\n",
    "                            nn.ReLU())\n",
    "\n",
    "        self.tran_cnn_layer2 = nn.Sequential(\n",
    "                            nn.ConvTranspose1d(16, output_dim, kernel_size = 2, stride = 2, padding=0),\n",
    "                            nn.Sigmoid())\n",
    "            \n",
    "            \n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        output = self.cnn_layer1(x)\n",
    "        # print(output.shape)\n",
    "        output = self.cnn_layer2(output)\n",
    "        # print(output.shape)\n",
    "        output = self.tran_cnn_layer1(output)\n",
    "        # print(output.shape)\n",
    "        output = self.tran_cnn_layer2(output)\n",
    "        # print(output.shape)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ece3007",
   "metadata": {},
   "source": [
    "## K Means Clustering with T-SNE Dimension Reduction Technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11b06164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = '1'\n",
    "import csv\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from scipy.cluster.vq import vq\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "class k_clustering():\n",
    "    def __init__(self, dataframe, dim):\n",
    "        self.df = dataframe\n",
    "        self.df_values = self.df.values\n",
    "        self.data = torch.tensor(self.df_values, dtype = torch.float32)\n",
    "        self.dim = dim\n",
    "    \n",
    "    def preprocess_with_autoencoder(self, t_sne_dim, decoder_dim= None):\n",
    "        #autoencoder input dimension\n",
    "        input_dim = self.data.size(1)\n",
    "\n",
    "        #output dimension 설정\n",
    "        if decoder_dim != None:\n",
    "            output_dim = decoder_dim\n",
    "        else:\n",
    "            output_dim = self.data.size(1)\n",
    "\n",
    "        #모델 통과\n",
    "        model = Autoencoder(input_dim, output_dim)\n",
    "        output = model(self.data)\n",
    "        print(\"autoencoder output shape:\", output.shape)\n",
    "\n",
    "        #tensor numpy로 변환\n",
    "        output = output.detach().numpy()\n",
    "        # print(output)\n",
    "\n",
    "        #T-SNE로 차원 축소\n",
    "        t_sne_model = TSNE(t_sne_dim)\n",
    "        t_sne_output = t_sne_model.fit_transform(output)\n",
    "        print(\"T-SNE output shape:\", t_sne_output.shape)\n",
    "\n",
    "\n",
    "        return t_sne_output\n",
    "\n",
    "\n",
    "    def preprocess_with_conv_autoencoder(self, t_sne_dim, decoder_dim=None):\n",
    "        #convolutional layer input을 위한 transpose\n",
    "        transpose_input = torch.transpose(self.data,1,0)\n",
    "        input_dim = transpose_input.size(0)\n",
    "\n",
    "        #추후 최적화 때 실험할 output dimension 설정\n",
    "        if decoder_dim != None:\n",
    "            output_dim = decoder_dim\n",
    "        else:\n",
    "            output_dim = transpose_input.size(0)\n",
    "\n",
    "        #모델 통과\n",
    "        model = ConvAutoEncoder(input_dim, output_dim)\n",
    "        decoded = model(transpose_input)\n",
    "        transpose_output = torch.transpose(decoded,1,0)\n",
    "        print(\"Conv Autoencoder output shape:\", transpose_output.shape)\n",
    "\n",
    "        #Numpy 변환\n",
    "        transpose_output = transpose_output.detach().numpy()\n",
    "\n",
    "        #output dimension for T-SNE\n",
    "        t_sne_model = TSNE(t_sne_dim)\n",
    "        t_sne_output = t_sne_model.fit_transform(transpose_output)\n",
    "        print(\"T-SNE output shape:\", t_sne_output.shape)\n",
    "\n",
    "        return t_sne_output\n",
    "    \n",
    "    def k_means_clustering(self, num_cluster, autoencoder_type):\n",
    "        clustering_input = None\n",
    "        if str(autoencoder_type) == (\"linear\" or \"Linear\"):\n",
    "            clustering_input = self.preprocess_with_autoencoder(2)\n",
    "        elif str(autoencoder_type) == (\"conv\" or \"Conv\"):\n",
    "            clustering_input = self.preprocess_with_conv_autoencoder(2)\n",
    "        else:\n",
    "            assert autoencoder_type == \"linear\" or autoencoder_type == \"conv\"\n",
    "\n",
    "        print(\"Clustering input dimension:\", clustering_input.shape)\n",
    "\n",
    "        k_means = KMeans(init= \"k-means++\", n_clusters = num_cluster)\n",
    "        k_means.fit(clustering_input)\n",
    "        cluster = k_means.predict(clustering_input)\n",
    "        labels = k_means.labels_\n",
    "        centers = k_means.cluster_centers_\n",
    "\n",
    "        #Medoid 찾기\n",
    "        closest, distances = vq(centers, clustering_input)\n",
    "        print(\"Closest Medoid Indexs are\", closest)\n",
    "\n",
    "        #2차원인 경우 그래프 그리기\n",
    "        if clustering_input.shape[1] == 2:\n",
    "            fig = plt.figure(figsize=(6, 4))\n",
    "            colors = plt.cm.Spectral(np.linspace(0, 1, len(set(labels))))\n",
    "            graph = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "            for k, col, close_idx in zip(range(6), colors, closest):\n",
    "                my_members = (labels == k)\n",
    "                if my_members[close_idx] == True:\n",
    "                    graph.plot(clustering_input[close_idx, 0], clustering_input[close_idx, 1], markerfacecolor=col, marker = 'h', markeredgecolor='b', markersize=20)\n",
    "                graph.plot(clustering_input[my_members, 0], clustering_input[my_members, 1], linestyle = 'None', mec ='k', mew= 0.5, markerfacecolor=col, marker='o', markersize = 5)\n",
    "                # graph.plot(input_data[close_center, 0], input_data[close_center, 1], 'o', markerfacecolor=col, markeredgecolor='k', markersize=20)\n",
    "\n",
    "            graph.set_title('K-Means')\n",
    "            graph.set_xticks(())\n",
    "            graph.set_yticks(())\n",
    "            plt.show()\n",
    "    \n",
    "    #실루엣 지수로 최적화\n",
    "    def optimize_clustering(self, autoencoder_type):\n",
    "        clustering_input = None\n",
    "        if str(autoencoder_type) == (\"linear\" or \"Linear\"):\n",
    "            clustering_input = self.preprocess_with_autoencoder(2)\n",
    "        elif str(autoencoder_type) == (\"conv\" or \"Conv\"):\n",
    "            clustering_input = self.preprocess_with_conv_autoencoder(2)\n",
    "        else:\n",
    "            assert autoencoder_type == \"linear\" or autoencoder_type == \"conv\"\n",
    "\n",
    "        print(\"Clustering input dimension:\", clustering_input.shape)\n",
    "\n",
    "        best_num_cluster = 0\n",
    "        best_silhouette_score = -1\n",
    "        k_range = range(4,10)\n",
    " \n",
    "        inertia_list = []\n",
    "\n",
    "        for k in k_range:\n",
    "            # k_means = MiniBatchKMeans(init=\"k-means++\", n_clusters=k, n_init=5, random_state = 1, batch_size=5)\n",
    "            k_means = KMeans(init=\"k-means++\", n_clusters=k, random_state = 1)\n",
    "            k_means.fit(clustering_input)\n",
    "            cluster = k_means.predict(clustering_input)\n",
    "\n",
    "            #silhouette score calculation\n",
    "            score = silhouette_score(clustering_input, cluster)\n",
    "            print('k:', k, \"silhouette score:\", score)\n",
    "\n",
    "            inertia = k_means.inertia_\n",
    "            inertia_list.append(inertia)\n",
    "\n",
    "            if score > best_silhouette_score:\n",
    "                best_num_cluster = k\n",
    "                best_silhouette_score = score\n",
    "\n",
    "        print(\"Best number of clusters:\", best_num_cluster, \"Score:\", best_silhouette_score)\n",
    "\n",
    "        return best_num_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885f40ca",
   "metadata": {},
   "source": [
    "## Clustering directly in preprocessed data excluding external data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "066c3249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced Dimension: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\14ZD\\miniconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:795: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\14ZD\\miniconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:805: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K: 4 Silhouette score: 0.36980572\n",
      "K: 5 Silhouette score: 0.35810667\n",
      "K: 6 Silhouette score: 0.3691869\n",
      "K: 7 Silhouette score: 0.38440692\n",
      "K: 8 Silhouette score: 0.37673187\n",
      "K: 9 Silhouette score: 0.38479546\n",
      "Reduced Dimension: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\14ZD\\miniconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:795: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\14ZD\\miniconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:805: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K: 4 Silhouette score: 0.27385148\n",
      "K: 5 Silhouette score: 0.28114492\n",
      "K: 6 Silhouette score: 0.29422444\n",
      "K: 7 Silhouette score: 0.29405937\n",
      "K: 8 Silhouette score: 0.2952418\n",
      "K: 9 Silhouette score: 0.28904483\n",
      "Best dim: 3 Best number of clusters: 9 Score: 0.38479546\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = '1'\n",
    "import csv\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from scipy.cluster.vq import vq\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('./data_final_ex.csv')\n",
    "df_values = df.values\n",
    "data = torch.tensor(df_values, dtype = torch.float32)\n",
    "dims = [2,3]\n",
    "\n",
    "best_num_cluster = 0\n",
    "best_silhouette_score = -1\n",
    "best_dim = 0\n",
    "\n",
    "for dim in dims:\n",
    "    print(\"Reduced Dimension:\", dim)\n",
    "    t_sne_model = TSNE(dim)\n",
    "    t_sne_output = t_sne_model.fit_transform(data)\n",
    "    k_range = range(4,10)\n",
    "\n",
    "    for k in k_range:\n",
    "\n",
    "        k_means = KMeans(init=\"k-means++\", n_clusters=k, random_state = 1)\n",
    "        k_means.fit(t_sne_output)\n",
    "        cluster = k_means.predict(t_sne_output)\n",
    "\n",
    "        #silhouette score calculation\n",
    "        score = silhouette_score(t_sne_output, cluster)\n",
    "        print('K:', k, \"Silhouette score:\", score)\n",
    "\n",
    "\n",
    "        if score > best_silhouette_score:\n",
    "            best_num_cluster = k\n",
    "            best_silhouette_score = score\n",
    "            best_dim = dim\n",
    "\n",
    "print(\"Best dim:\", best_dim, \"Best number of clusters:\", best_num_cluster, \"Score:\", best_silhouette_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f65d1a0",
   "metadata": {},
   "source": [
    "## Clustering directly in preprocessed data including external data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79b9514a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.0.10-py2.py3-none-any.whl (242 kB)\n",
      "Collecting et-xmlfile\n",
      "  Downloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-1.1.0 openpyxl-3.0.10\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6919a704",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\14ZD\\miniconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:795: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\14ZD\\miniconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:805: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nTSNE does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14844\\2366965285.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdims\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mt_sne_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0mt_sne_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt_sne_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[0mk_range\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m   1115\u001b[0m             \u001b[0mEmbedding\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlow\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mdimensional\u001b[0m \u001b[0mspace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m         \"\"\"\n\u001b[1;32m-> 1117\u001b[1;33m         \u001b[0membedding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1118\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, skip_num_points)\u001b[0m\n\u001b[0;32m    836\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"'learning_rate' must be a positive number or 'auto'.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    837\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"barnes_hut\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 838\u001b[1;33m             X = self._validate_data(\n\u001b[0m\u001b[0;32m    839\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    840\u001b[0m                 \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"csr\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    575\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    576\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 577\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"X\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    578\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    897\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    898\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 899\u001b[1;33m             _assert_all_finite(\n\u001b[0m\u001b[0;32m    900\u001b[0m                 \u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    901\u001b[0m                 \u001b[0minput_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    144\u001b[0m                     \u001b[1;34m\"#estimators-that-handle-nan-values\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m                 )\n\u001b[1;32m--> 146\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[1;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nTSNE does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = '1'\n",
    "import csv\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from scipy.cluster.vq import vq\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('./data_final.csv')\n",
    "df_values = df.values\n",
    "data = torch.tensor(df_values, dtype = torch.float32)\n",
    "dims = [2,3]\n",
    "\n",
    "best_num_cluster = 0\n",
    "best_silhouette_score = -1\n",
    "best_dim = 0\n",
    "\n",
    "for dim in dims:\n",
    "    t_sne_model = TSNE(dim)\n",
    "    t_sne_output = t_sne_model.fit_transform(data)\n",
    "    k_range = range(4,10)\n",
    "\n",
    "    for k in k_range:\n",
    "\n",
    "        k_means = KMeans(init=\"k-means++\", n_clusters=k, random_state = 1)\n",
    "        k_means.fit(t_sne_output)\n",
    "        cluster = k_means.predict(t_sne_output)\n",
    "\n",
    "        #silhouette score calculation\n",
    "        score = silhouette_score(t_sne_output, cluster)\n",
    "        print('K:', k, \"Silhouette score:\", score)\n",
    "\n",
    "\n",
    "        if score > best_silhouette_score:\n",
    "            best_num_cluster = k\n",
    "            best_silhouette_score = score\n",
    "            best_dim = dim\n",
    "\n",
    "print(\"Best dim:\", best_dim, \"Best number of clusters:\", best_num_cluster, \"Score:\", best_silhouette_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8010fada",
   "metadata": {},
   "source": [
    "## Clustering in preprocessed data through Linear Autoencoder excluding external data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74531de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data_final_ex.csv')\n",
    "dims = [2,3]\n",
    "for dim in dims:\n",
    "    print(\"Reduced Dimension:\", dim)\n",
    "    model = k_clustering(df, dim)\n",
    "    #parameter = \"linear\" or \"conv\"\n",
    "    optimal_num_cluster = model.optimize_clustering('linear')\n",
    "    # optimal_num_cluster = 4\n",
    "    # model.k_means_clustering(optimal_num_cluster, \"conv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8e4fc8",
   "metadata": {},
   "source": [
    "## Clustering in preprocessed data through Conv Autoencoder excluding external data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05d8558",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data_final_ex.csv')\n",
    "dims = [2,3]\n",
    "for dim in dims:\n",
    "    print(\"Reduced Dimension:\", dim)\n",
    "    model = k_clustering(df, dim)\n",
    "    #parameter = \"linear\" or \"conv\"\n",
    "    optimal_num_cluster = model.optimize_clustering('conv')\n",
    "    # optimal_num_cluster = 4\n",
    "    # model.k_means_clustering(optimal_num_cluster, \"conv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b637ebae",
   "metadata": {},
   "source": [
    "## Clustering in preprocessed data through Linear Autoencoder including external data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92443ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data_final.csv')\n",
    "dims = [2,3]\n",
    "for dim in dims:\n",
    "    print(\"Reduced Dimension:\", dim)\n",
    "    model = k_clustering(df, dim)\n",
    "    #parameter = \"linear\" or \"conv\"\n",
    "    optimal_num_cluster = model.optimize_clustering('linear')\n",
    "    # optimal_num_cluster = 4\n",
    "    # model.k_means_clustering(optimal_num_cluster, \"conv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e956f06c",
   "metadata": {},
   "source": [
    "## Clustering in preprocessed data through Conv Autoencoder including external data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17d1215",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data_final.csv')\n",
    "dims = [2,3]\n",
    "for dim in dims:\n",
    "    print(\"Reduced Dimension:\", dim)\n",
    "    model = k_clustering(df, dim)\n",
    "    #parameter = \"linear\" or \"conv\"\n",
    "    optimal_num_cluster = model.optimize_clustering('conv')\n",
    "    # optimal_num_cluster = 4\n",
    "    # model.k_means_clustering(optimal_num_cluster, \"conv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cb2ca3",
   "metadata": {},
   "source": [
    "## Optimal Clustering Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ecbed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "start =time.time()\n",
    "df = pd.read_csv('./data_final.csv')\n",
    "model = k_clustering(df, 2)\n",
    "#parameter = \"linear\" or \"conv\"\n",
    "# optimal_num_cluster = model.optimize_clustering('conv')\n",
    "optimal_num_cluster = 4\n",
    "model.k_means_clustering(optimal_num_cluster, \"conv\")\n",
    "print(\"Elapsed Time:\", time.time() - start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
